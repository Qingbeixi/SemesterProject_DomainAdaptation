VAERegressor(
  (conv1): Conv1d(20, 10, kernel_size=(3,), stride=(1,), padding=(1,))
  (bn1): BatchNorm1d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (dropout1): Dropout(p=0.2, inplace=False)
  (pool1): MaxPool1d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  (conv2): Conv1d(10, 10, kernel_size=(3,), stride=(1,), padding=(1,))
  (bn2): BatchNorm1d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (dropout2): Dropout(p=0.2, inplace=False)
  (pool2): MaxPool1d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  (conv3): Conv1d(10, 5, kernel_size=(3,), stride=(1,), padding=(1,))
  (bn3): BatchNorm1d(5, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (fc_mu): Linear(in_features=60, out_features=5, bias=True)
  (fc_logvar): Linear(in_features=60, out_features=5, bias=True)
  (fc1): Linear(in_features=5, out_features=250, bias=True)
  (conv4): ConvTranspose1d(5, 10, kernel_size=(3,), stride=(1,), padding=(1,))
  (bn4): BatchNorm1d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (dropout4): Dropout(p=0.2, inplace=False)
  (conv5): ConvTranspose1d(10, 20, kernel_size=(3,), stride=(1,), padding=(1,))
  (fc4): Linear(in_features=250, out_features=1, bias=True)
  (sigmoid): Sigmoid()
)
We apply the domain adaptation for m to l
Train shape (30918, 50, 20)
Test shape (49018, 50, 20)
Epoch 1/40, Train_RMSELoss: 0.2920, Test_RMSELoss:0.2922
Epoch 2/40, Train_RMSELoss: 0.2906, Test_RMSELoss:0.2912
Epoch 3/40, Train_RMSELoss: 0.2902, Test_RMSELoss:0.2908
Epoch 4/40, Train_RMSELoss: 0.2901, Test_RMSELoss:0.2911
Epoch 5/40, Train_RMSELoss: 0.2900, Test_RMSELoss:0.2909
Epoch 6/40, Train_RMSELoss: 0.2900, Test_RMSELoss:0.2906
Epoch 7/40, Train_RMSELoss: 0.2901, Test_RMSELoss:0.2906
Epoch 8/40, Train_RMSELoss: 0.2900, Test_RMSELoss:0.2911
Epoch 9/40, Train_RMSELoss: 0.2850, Test_RMSELoss:0.2276
Epoch 10/40, Train_RMSELoss: 0.1669, Test_RMSELoss:0.1608
Epoch 11/40, Train_RMSELoss: 0.1358, Test_RMSELoss:0.1664
Epoch 12/40, Train_RMSELoss: 0.1225, Test_RMSELoss:0.1332
Epoch 13/40, Train_RMSELoss: 0.1224, Test_RMSELoss:0.1513
Epoch 14/40, Train_RMSELoss: 0.1196, Test_RMSELoss:0.1196
Epoch 15/40, Train_RMSELoss: 0.1179, Test_RMSELoss:0.1232
Epoch 16/40, Train_RMSELoss: 0.1160, Test_RMSELoss:0.1238
Epoch 17/40, Train_RMSELoss: 0.1180, Test_RMSELoss:0.1103
Epoch 18/40, Train_RMSELoss: 0.1139, Test_RMSELoss:0.1331
Epoch 19/40, Train_RMSELoss: 0.1165, Test_RMSELoss:0.1365
Epoch 20/40, Train_RMSELoss: 0.1124, Test_RMSELoss:0.1089
Epoch 21/40, Train_RMSELoss: 0.1154, Test_RMSELoss:0.1077
Epoch 22/40, Train_RMSELoss: 0.1126, Test_RMSELoss:0.1140
Epoch 23/40, Train_RMSELoss: 0.1128, Test_RMSELoss:0.1047
Epoch 24/40, Train_RMSELoss: 0.1142, Test_RMSELoss:0.1083
Epoch 25/40, Train_RMSELoss: 0.1108, Test_RMSELoss:0.1014
Epoch 26/40, Train_RMSELoss: 0.1108, Test_RMSELoss:0.1158
Epoch 27/40, Train_RMSELoss: 0.1119, Test_RMSELoss:0.0967
Epoch 28/40, Train_RMSELoss: 0.1106, Test_RMSELoss:0.1089
Epoch 29/40, Train_RMSELoss: 0.1096, Test_RMSELoss:0.0958
Epoch 30/40, Train_RMSELoss: 0.1100, Test_RMSELoss:0.0986
Epoch 31/40, Train_RMSELoss: 0.1100, Test_RMSELoss:0.1029
Epoch 32/40, Train_RMSELoss: 0.1092, Test_RMSELoss:0.0962
Epoch 33/40, Train_RMSELoss: 0.1118, Test_RMSELoss:0.0969
Epoch 34/40, Train_RMSELoss: 0.1093, Test_RMSELoss:0.1084
Epoch 35/40, Train_RMSELoss: 0.1093, Test_RMSELoss:0.0935
Epoch 36/40, Train_RMSELoss: 0.1079, Test_RMSELoss:0.0926
Epoch 37/40, Train_RMSELoss: 0.1085, Test_RMSELoss:0.0978
Epoch 38/40, Train_RMSELoss: 0.1096, Test_RMSELoss:0.0987
Epoch 39/40, Train_RMSELoss: 0.1075, Test_RMSELoss:0.0960
Epoch 40/40, Train_RMSELoss: 0.1087, Test_RMSELoss:0.0931
{"Test Loss for unit 6": 5.129042148590088, "Test Loss for unit 8": 5.235971450805664, "Test Loss for unit 10": 4.834044933319092, "Test Loss for unit 11": 4.814493179321289, "Test Loss for unit 13": 5.297063827514648}