FullModel(
  (feature_extractor): FeatureExtractor(
    (conv1d_1): Conv1d(20, 32, kernel_size=(3,), stride=(1,), padding=(1,))
    (bn_1): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (conv1d_2): Conv1d(32, 64, kernel_size=(3,), stride=(1,), padding=(1,))
    (bn_2): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (conv1d_3): Conv1d(64, 128, kernel_size=(3,), stride=(1,), padding=(1,))
    (bn_3): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (conv1d_4): Conv1d(128, 256, kernel_size=(3,), stride=(1,), padding=(1,))
    (bn_4): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (conv1d_5): Conv1d(256, 512, kernel_size=(3,), stride=(1,), padding=(1,))
    (bn_5): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (pool): MaxPool1d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
    (dropout): Dropout(p=0.5, inplace=False)
    (relu): ReLU()
    (flatten): Flatten(start_dim=1, end_dim=-1)
    (dense_layer_1): Linear(in_features=512, out_features=256, bias=True)
    (bn_dense_1): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (dense_layer_2): Linear(in_features=256, out_features=128, bias=True)
    (bn_dense_2): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  )
  (regressor): Regressor(
    (fc1): Linear(in_features=128, out_features=1, bias=True)
  )
)
We apply the domain adaptation for s to l
Train shape (18224, 50, 20)
Test shape (49018, 50, 20)
Epoch 1/30, Train_RMSELoss: 0.4712, Test_RMSELoss:0.3507
Epoch 2/30, Train_RMSELoss: 0.3345, Test_RMSELoss:0.3218
Epoch 3/30, Train_RMSELoss: 0.3146, Test_RMSELoss:0.3113
Epoch 4/30, Train_RMSELoss: 0.3068, Test_RMSELoss:0.3051
Epoch 5/30, Train_RMSELoss: 0.3008, Test_RMSELoss:0.3047
Epoch 6/30, Train_RMSELoss: 0.2934, Test_RMSELoss:0.2982
Epoch 7/30, Train_RMSELoss: 0.2246, Test_RMSELoss:0.2424
Epoch 8/30, Train_RMSELoss: 0.1739, Test_RMSELoss:0.1943
Epoch 9/30, Train_RMSELoss: 0.1625, Test_RMSELoss:0.2136
Epoch 10/30, Train_RMSELoss: 0.1545, Test_RMSELoss:0.2063
Epoch 11/30, Train_RMSELoss: 0.1448, Test_RMSELoss:0.2018
Epoch 12/30, Train_RMSELoss: 0.1404, Test_RMSELoss:0.1809
Epoch 13/30, Train_RMSELoss: 0.1449, Test_RMSELoss:0.2174
Epoch 14/30, Train_RMSELoss: 0.1375, Test_RMSELoss:0.1962
Epoch 15/30, Train_RMSELoss: 0.1339, Test_RMSELoss:0.1805
Epoch 16/30, Train_RMSELoss: 0.1348, Test_RMSELoss:0.2273
Epoch 17/30, Train_RMSELoss: 0.1326, Test_RMSELoss:0.1888
Epoch 18/30, Train_RMSELoss: 0.1314, Test_RMSELoss:0.1908
Epoch 19/30, Train_RMSELoss: 0.1300, Test_RMSELoss:0.1878
Epoch 20/30, Train_RMSELoss: 0.1294, Test_RMSELoss:0.1837
Epoch 21/30, Train_RMSELoss: 0.1253, Test_RMSELoss:0.1825
Epoch 22/30, Train_RMSELoss: 0.1248, Test_RMSELoss:0.2007
Epoch 23/30, Train_RMSELoss: 0.1237, Test_RMSELoss:0.2146
Epoch 24/30, Train_RMSELoss: 0.1244, Test_RMSELoss:0.1904
Epoch 25/30, Train_RMSELoss: 0.1225, Test_RMSELoss:0.1860
Epoch 26/30, Train_RMSELoss: 0.1225, Test_RMSELoss:0.1811
Epoch 27/30, Train_RMSELoss: 0.1211, Test_RMSELoss:0.2042
Epoch 28/30, Train_RMSELoss: 0.1208, Test_RMSELoss:0.1792
Epoch 29/30, Train_RMSELoss: 0.1194, Test_RMSELoss:0.1990
Epoch 30/30, Train_RMSELoss: 0.1178, Test_RMSELoss:0.2025
