FullModel(
  (feature_extractor): FeatureExtractor(
    (conv1d_1): Conv1d(20, 32, kernel_size=(3,), stride=(1,), padding=(1,))
    (bn_1): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (conv1d_2): Conv1d(32, 64, kernel_size=(3,), stride=(1,), padding=(1,))
    (bn_2): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (conv1d_3): Conv1d(64, 128, kernel_size=(3,), stride=(1,), padding=(1,))
    (bn_3): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (conv1d_4): Conv1d(128, 256, kernel_size=(3,), stride=(1,), padding=(1,))
    (bn_4): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (conv1d_5): Conv1d(256, 512, kernel_size=(3,), stride=(1,), padding=(1,))
    (bn_5): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (pool): MaxPool1d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
    (dropout): Dropout(p=0.5, inplace=False)
    (relu): ReLU()
    (flatten): Flatten(start_dim=1, end_dim=-1)
    (dense_layer_1): Linear(in_features=512, out_features=256, bias=True)
    (bn_dense_1): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (dense_layer_2): Linear(in_features=256, out_features=128, bias=True)
    (bn_dense_2): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  )
  (regressor): Regressor(
    (fc1): Linear(in_features=128, out_features=1, bias=True)
  )
)
We apply the domain adaptation for m to l
Train shape (30918, 50, 20)
Test shape (49018, 50, 20)
Epoch 1/30, Train_RMSELoss: 0.4127, Test_RMSELoss:0.3181
Epoch 2/30, Train_RMSELoss: 0.3092, Test_RMSELoss:0.3014
Epoch 3/30, Train_RMSELoss: 0.2567, Test_RMSELoss:0.2291
Epoch 4/30, Train_RMSELoss: 0.1816, Test_RMSELoss:0.1743
Epoch 5/30, Train_RMSELoss: 0.1620, Test_RMSELoss:0.1787
Epoch 6/30, Train_RMSELoss: 0.1529, Test_RMSELoss:0.1786
Epoch 7/30, Train_RMSELoss: 0.1457, Test_RMSELoss:0.2196
Epoch 8/30, Train_RMSELoss: 0.1406, Test_RMSELoss:0.1518
Epoch 9/30, Train_RMSELoss: 0.1353, Test_RMSELoss:0.1431
Epoch 10/30, Train_RMSELoss: 0.1338, Test_RMSELoss:0.1449
Epoch 11/30, Train_RMSELoss: 0.1310, Test_RMSELoss:0.1826
Epoch 12/30, Train_RMSELoss: 0.1263, Test_RMSELoss:0.1359
Epoch 13/30, Train_RMSELoss: 0.1313, Test_RMSELoss:0.1444
Epoch 14/30, Train_RMSELoss: 0.1245, Test_RMSELoss:0.1727
Epoch 15/30, Train_RMSELoss: 0.1281, Test_RMSELoss:0.1404
Epoch 16/30, Train_RMSELoss: 0.1236, Test_RMSELoss:0.1672
Epoch 17/30, Train_RMSELoss: 0.1232, Test_RMSELoss:0.1313
Epoch 18/30, Train_RMSELoss: 0.1228, Test_RMSELoss:0.1366
Epoch 19/30, Train_RMSELoss: 0.1207, Test_RMSELoss:0.1518
Epoch 20/30, Train_RMSELoss: 0.1223, Test_RMSELoss:0.1201
Epoch 21/30, Train_RMSELoss: 0.1192, Test_RMSELoss:0.1222
Epoch 22/30, Train_RMSELoss: 0.1188, Test_RMSELoss:0.1201
Epoch 23/30, Train_RMSELoss: 0.1167, Test_RMSELoss:0.1214
Epoch 24/30, Train_RMSELoss: 0.1181, Test_RMSELoss:0.1164
Epoch 25/30, Train_RMSELoss: 0.1196, Test_RMSELoss:0.1335
Epoch 26/30, Train_RMSELoss: 0.1197, Test_RMSELoss:0.1173
Epoch 27/30, Train_RMSELoss: 0.1180, Test_RMSELoss:0.1336
Epoch 28/30, Train_RMSELoss: 0.1174, Test_RMSELoss:0.1425
Epoch 29/30, Train_RMSELoss: 0.1177, Test_RMSELoss:0.1448
Epoch 30/30, Train_RMSELoss: 0.1160, Test_RMSELoss:0.1351
{"Test Loss for unit 6": 8.457636833190918, "Test Loss for unit 8": 9.658101081848145, "Test Loss for unit 10": 8.470338821411133, "Test Loss for unit 11": 7.19122314453125, "Test Loss for unit 13": 10.026305198669434}