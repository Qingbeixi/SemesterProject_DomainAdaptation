VAERegressor(
  (conv1): Conv1d(20, 10, kernel_size=(3,), stride=(1,), padding=(1,))
  (bn1): BatchNorm1d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (dropout1): Dropout(p=0.2, inplace=False)
  (pool1): MaxPool1d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  (conv2): Conv1d(10, 10, kernel_size=(3,), stride=(1,), padding=(1,))
  (bn2): BatchNorm1d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (dropout2): Dropout(p=0.2, inplace=False)
  (pool2): MaxPool1d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  (conv3): Conv1d(10, 5, kernel_size=(3,), stride=(1,), padding=(1,))
  (bn3): BatchNorm1d(5, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (fc_mu): Linear(in_features=60, out_features=5, bias=True)
  (fc_logvar): Linear(in_features=60, out_features=5, bias=True)
  (fc1): Linear(in_features=5, out_features=250, bias=True)
  (conv4): ConvTranspose1d(5, 10, kernel_size=(3,), stride=(1,), padding=(1,))
  (bn4): BatchNorm1d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (dropout4): Dropout(p=0.2, inplace=False)
  (conv5): ConvTranspose1d(10, 20, kernel_size=(3,), stride=(1,), padding=(1,))
  (fc4): Linear(in_features=250, out_features=1, bias=True)
  (sigmoid): Sigmoid()
)
We apply the domain adaptation for s to m
Train shape (18224, 50, 20)
Test shape (30918, 50, 20)
Epoch 1/40, Train_RMSELoss: 0.2477, Test_RMSELoss:0.2396
Epoch 2/40, Train_RMSELoss: 0.1932, Test_RMSELoss:0.2695
Epoch 3/40, Train_RMSELoss: 0.1761, Test_RMSELoss:0.2156
Epoch 4/40, Train_RMSELoss: 0.1606, Test_RMSELoss:0.1795
Epoch 5/40, Train_RMSELoss: 0.1450, Test_RMSELoss:0.1711
Epoch 6/40, Train_RMSELoss: 0.1372, Test_RMSELoss:0.1977
Epoch 7/40, Train_RMSELoss: 0.1363, Test_RMSELoss:0.1489
Epoch 8/40, Train_RMSELoss: 0.1241, Test_RMSELoss:0.1795
Epoch 9/40, Train_RMSELoss: 0.1259, Test_RMSELoss:0.2014
Epoch 10/40, Train_RMSELoss: 0.1241, Test_RMSELoss:0.1757
Epoch 11/40, Train_RMSELoss: 0.1220, Test_RMSELoss:0.1476
Epoch 12/40, Train_RMSELoss: 0.1186, Test_RMSELoss:0.1419
Epoch 13/40, Train_RMSELoss: 0.1147, Test_RMSELoss:0.1372
Epoch 14/40, Train_RMSELoss: 0.1130, Test_RMSELoss:0.1352
Epoch 15/40, Train_RMSELoss: 0.1101, Test_RMSELoss:0.1377
Epoch 16/40, Train_RMSELoss: 0.1150, Test_RMSELoss:0.1470
Epoch 17/40, Train_RMSELoss: 0.1125, Test_RMSELoss:0.1313
Epoch 18/40, Train_RMSELoss: 0.1092, Test_RMSELoss:0.1280
Epoch 19/40, Train_RMSELoss: 0.1097, Test_RMSELoss:0.1459
Epoch 20/40, Train_RMSELoss: 0.1108, Test_RMSELoss:0.1748
Epoch 21/40, Train_RMSELoss: 0.1101, Test_RMSELoss:0.1560
Epoch 22/40, Train_RMSELoss: 0.1118, Test_RMSELoss:0.1412
Epoch 23/40, Train_RMSELoss: 0.1094, Test_RMSELoss:0.1377
Epoch 24/40, Train_RMSELoss: 0.1137, Test_RMSELoss:0.1434
Epoch 25/40, Train_RMSELoss: 0.1076, Test_RMSELoss:0.1297
Epoch 26/40, Train_RMSELoss: 0.1065, Test_RMSELoss:0.1382
Epoch 27/40, Train_RMSELoss: 0.1071, Test_RMSELoss:0.1606
Epoch 28/40, Train_RMSELoss: 0.1077, Test_RMSELoss:0.1403
Epoch 29/40, Train_RMSELoss: 0.1061, Test_RMSELoss:0.1572
Epoch 30/40, Train_RMSELoss: 0.1050, Test_RMSELoss:0.1275
Epoch 31/40, Train_RMSELoss: 0.1051, Test_RMSELoss:0.1626
Epoch 32/40, Train_RMSELoss: 0.1066, Test_RMSELoss:0.1280
Epoch 33/40, Train_RMSELoss: 0.1029, Test_RMSELoss:0.1495
Epoch 34/40, Train_RMSELoss: 0.1063, Test_RMSELoss:0.1237
Epoch 35/40, Train_RMSELoss: 0.1039, Test_RMSELoss:0.1355
Epoch 36/40, Train_RMSELoss: 0.1058, Test_RMSELoss:0.1293
Epoch 37/40, Train_RMSELoss: 0.1070, Test_RMSELoss:0.1428
Epoch 38/40, Train_RMSELoss: 0.1037, Test_RMSELoss:0.1254
Epoch 39/40, Train_RMSELoss: 0.1018, Test_RMSELoss:0.1395
Epoch 40/40, Train_RMSELoss: 0.1032, Test_RMSELoss:0.1394
{"Test Loss for unit 2": 8.03402328491211, "Test Loss for unit 3": 7.679384231567383, "Test Loss for unit 4": 6.20404577255249, "Test Loss for unit 7": 9.19710922241211, "Test Loss for unit 15": 7.655881881713867}